[
  {
    "objectID": "math-437-project-template (1).html#main-objective",
    "href": "math-437-project-template (1).html#main-objective",
    "title": "Replace this with an interesting title",
    "section": "Main Objective",
    "text": "Main Objective"
  },
  {
    "objectID": "math-437-project-template (1).html#packages-used-in-this-analysis",
    "href": "math-437-project-template (1).html#packages-used-in-this-analysis",
    "title": "Replace this with an interesting title",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nlibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rsample)\n\n\n\n\nPackage\nUse\n\n\n\n\nhere\nto easily load and save data\n\n\nreadr\nto import the CSV file data\n\n\ndplyr\nto massage and summarize data\n\n\nrsample\nto split data into training and test sets\n\n\nggplot2\nto create nice-looking and informative graphs"
  },
  {
    "objectID": "math-437-project-template (1).html#data-description",
    "href": "math-437-project-template (1).html#data-description",
    "title": "Replace this with an interesting title",
    "section": "Data Description",
    "text": "Data Description\nI am using the fire incident data available at the CAL FIRE website. CalFIRE (California Department of Forestry and Fire Protection) is California’s state agency responsible for fire protection in State Responsibility Areas (SRAs), which include over 31 million acres of wildlands and areas with significant natural resources. CAL FIRE also provides emergency services in cooperation with local governments and handles forest management, fire prevention, and resource protection.\nAll of the agency’s information comes from the firelines and must be approved by the Incident Commander in charge of managing the incident prior to release. The data then gets compiled by their staff and published in their website for the public to access. In their incidents page, there is a Current Emergency Incidents section where it shows fire related information such as evacuation orders and acres burned through Earthstar Geographics powered by Esri, Genasys and Perimeter. However, since I needed the whole historical fire data set to perform my analysis , I collected the CSV file, they offered.\nThe main reasons why they collect the fire incident data includes fastening fire responsiveness, prevention of future fires, to inform the public through statistics, and to create an incentive for more funding towards fire prevention.\n\nData Limitations\nCalFIRE makes it clear in their disclaimer that the data provided is intended for general reference rather than detailed scientific or emergency analysis. Therefore, important limitations must be considered when interpreting my model’s conclusions.\n(1) Bias in data collection: One possible source of bias is the fact that CalFIRE reportedly shared that a significant amount of small fires could go under reported, even more so if the incident burns less than 10 acres. Furthermore, another source of bias can be explained by how I performed variable selection during the modeling process. Since I got the data before specifing my linear model, I must account for selection bias.\n(2) Variables Mismatches: The outcome variable in our linear model, acres_burned is the only variable being used in my analysis to define the severity of the fire incidents. However, we are excluding so many crucial variables that influence a fire severity in the real-world such as structures destroyed, rural/urban differences, fire escalation, lack of firefighters, area accessibility, injuries, fatalities, more expensive/cheaper neighborhoods, total loss, and more. Plus, the variables incident_season and incident_county are very broad. I am not accounting for local weather, vegetation, source of fire, and human factors that could influence a fire behavior in the real-world.\n(3) Model Transferrability: My model is only accounting for fires in California in the past 12 years. Hence, the model might be inappropriate for other regions that present different weather, air humidity, terrain, biodiversity, population demographics, government funding, and more."
  },
  {
    "objectID": "math-437-project-template (1).html#data-wrangling-optional-section",
    "href": "math-437-project-template (1).html#data-wrangling-optional-section",
    "title": "Replace this with an interesting title",
    "section": "Data Wrangling (Optional Section)",
    "text": "Data Wrangling (Optional Section)"
  },
  {
    "objectID": "math-437-project-template (1).html#exploratory-data-analysis",
    "href": "math-437-project-template (1).html#exploratory-data-analysis",
    "title": "Replace this with an interesting title",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis"
  },
  {
    "objectID": "math-437-project-template (1).html#modeling",
    "href": "math-437-project-template (1).html#modeling",
    "title": "Replace this with an interesting title",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "math-437-project-template (1).html#insights",
    "href": "math-437-project-template (1).html#insights",
    "title": "Replace this with an interesting title",
    "section": "Insights",
    "text": "Insights\n\nLimitations and Future Work\n\n\nReflection (Optional Subsection)"
  },
  {
    "objectID": "Final Project Website/projects/index.html",
    "href": "Final Project Website/projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting House Prices with Machine Learning\n\n\n\nPython\n\n\nMachine Learning\n\n\nData Cleaning\n\n\n\nThis project involves using machine learning algorithms to predict house prices based on various features such as location, size, and amenities. It includes data cleaning, feature engineering, and model selection.\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segmentation Using Clustering Techniques\n\n\n\nR\n\n\nMachine Learning\n\n\nClustering\n\n\nStatistical Modelling\n\n\n\nThis project focuses on segmenting customers into different groups based on their purchasing behavior and demographics. It uses clustering algorithms like K-means and hierarchical clustering to identify distinct customer segments.\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Global CO2 Emissions\n\n\n\nR\n\n\nData Visualization\n\n\nEnvironmental Science\n\n\n\nThis project involves creating visualizations to show trends in global CO2 emissions over time. It includes data extraction from public databases, data cleaning, and using visualization libraries to create interactive charts and graphs.\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Final Project Website/blog/second-post/index.html",
    "href": "Final Project Website/blog/second-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Amet cursus sit amet dictum sit amet. Eget duis at tellus at urna condimentum. Convallis aenean et tortor at risus viverra. Tincidunt ornare massa eget egestas purus viverra accumsan. Et malesuada fames ac turpis egestas. At imperdiet dui accumsan sit amet. Ut ornare lectus sit amet est placerat. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Duis ultricies lacus sed turpis tincidunt id aliquet risus. Mattis enim ut tellus elementum sagittis. Dui id ornare arcu odio ut. Natoque penatibus et magnis dis. Libero justo laoreet sit amet cursus sit. Sed faucibus turpis in eu. Tempus iaculis urna id volutpat lacus laoreet.\nPhasellus vestibulum lorem sed risus. Eget felis eget nunc lobortis mattis. Sit amet aliquam id diam maecenas ultricies. Egestas maecenas pharetra convallis posuere morbi. Etiam erat velit scelerisque in dictum non consectetur a erat. Cras fermentum odio eu feugiat pretium nibh ipsum consequat. Viverra accumsan in nisl nisi scelerisque. Et netus et malesuada fames ac. Amet tellus cras adipiscing enim eu turpis egestas pretium aenean. Eget lorem dolor sed viverra ipsum nunc aliquet. Ultrices dui sapien eget mi proin sed libero enim sed. Ultricies mi eget mauris pharetra et ultrices neque. Ipsum suspendisse ultrices gravida dictum. A arcu cursus vitae congue mauris rhoncus aenean vel. Gravida arcu ac tortor dignissim convallis. Nulla posuere sollicitudin aliquam ultrices."
  },
  {
    "objectID": "Analysis.html",
    "href": "Analysis.html",
    "title": "Understanding Fire Severity in California",
    "section": "",
    "text": "The devastating series of wildfires in Los Angeles County earlier this January inspired me to focus my final project on analyzing fire incidents across California counties over the past 12 years. This project aims to explore key relationships between predictor variables and the severity of wildfires, measured by acres burned, while also developing a predictive model to anticipate future fire impacts. In addition to model-building, I’m investigating several important inference questions that reflect real-world concerns:\n\nDoes the severity of a wildfire depend on the season of the year?\nAre wildfires in California worsening over time in terms of severity?\nDoes a Complex fire in California influence fire severity?\n\nBy addressing these questions, I hope to better understand the patterns behind wildfire behavior—an understanding that could be crucial in shaping actuarial risk models and insurance strategies in the future.\nThis problem is especially compelling to me because I am majoring in Mathematics with a concentration in Actuarial Science and intend to pursue the Casualty Actuarial Society (CAS) track within the Property and Casualty insurance industry. As someone preparing for a career focused on analyzing risk and uncertainty, I believe that understanding the severity of fires in California, even if only measured by acres burned, is relevant for my career. Wildfires play a significant role in shaping Homeowners Insurance, both in terms of pricing and coverage availability. By studying this issue more deeply, I hope to gain insights that will one day allow me to develop more accurate predictive models and contribute to effective risk management strategies. Understanding these patterns isn’t just academically interesting—it directly aligns with my long-term professional goals of helping insurers and communities better prepare for and respond to natural disasters.\nNote: For context, it is important to note the key difference between the severity being measured in this project and the severity in insurance rate making.\nIn this project, we are defining severity as how destructive a fire is based on the length it impacted, disregarding other factors:\n\\[\n\\text{Severity} = \\text{Amount of acres burned by a fire}\\\n\\]\nIn insurance rate making, severity refers to the average cost of a claim:\n\\[\n\\text{Severity} = \\frac{\\text{Total Loss Amount}}{\\text{Number of Claims}}\n\\]\nOBS: This clarification will definitely be helpful for actuaries and insurance professionals."
  },
  {
    "objectID": "Analysis.html#motivation-and-context",
    "href": "Analysis.html#motivation-and-context",
    "title": "Understanding Fire Severity in California",
    "section": "",
    "text": "The devastating series of wildfires in Los Angeles County earlier this January inspired me to focus my final project on analyzing fire incidents across California counties over the past 12 years. This project aims to explore key relationships between predictor variables and the severity of wildfires, measured by acres burned, while also developing a predictive model to anticipate future fire impacts. In addition to model-building, I’m investigating several important inference questions that reflect real-world concerns:\n\nDoes the severity of a wildfire depend on the season of the year?\nAre wildfires in California worsening over time in terms of severity?\nDoes a Complex fire in California influence fire severity?\n\nBy addressing these questions, I hope to better understand the patterns behind wildfire behavior—an understanding that could be crucial in shaping actuarial risk models and insurance strategies in the future.\nThis problem is especially compelling to me because I am majoring in Mathematics with a concentration in Actuarial Science and intend to pursue the Casualty Actuarial Society (CAS) track within the Property and Casualty insurance industry. As someone preparing for a career focused on analyzing risk and uncertainty, I believe that understanding the severity of fires in California, even if only measured by acres burned, is relevant for my career. Wildfires play a significant role in shaping Homeowners Insurance, both in terms of pricing and coverage availability. By studying this issue more deeply, I hope to gain insights that will one day allow me to develop more accurate predictive models and contribute to effective risk management strategies. Understanding these patterns isn’t just academically interesting—it directly aligns with my long-term professional goals of helping insurers and communities better prepare for and respond to natural disasters.\nNote: For context, it is important to note the key difference between the severity being measured in this project and the severity in insurance rate making.\nIn this project, we are defining severity as how destructive a fire is based on the length it impacted, disregarding other factors:\n\\[\n\\text{Severity} = \\text{Amount of acres burned by a fire}\\\n\\]\nIn insurance rate making, severity refers to the average cost of a claim:\n\\[\n\\text{Severity} = \\frac{\\text{Total Loss Amount}}{\\text{Number of Claims}}\n\\]\nOBS: This clarification will definitely be helpful for actuaries and insurance professionals."
  },
  {
    "objectID": "Analysis.html#main-objective",
    "href": "Analysis.html#main-objective",
    "title": "Understanding Fire Severity in California",
    "section": "Main Objective",
    "text": "Main Objective\nI believe the insights gained from my analysis should be beneficial for my growth as a data scientist. Addressing this issue allows me to connect my career aspirations with a real opportunity to showcase the data analysis approaches I have learned in this class. The inferences drawn from my project should be merely illustrative and not applied in the real world since my analysis have so many limitations. Although, I believe this project have the potential to incentivize other data scientists, statisticians, and actuaries to help this issue by working on better predictive models and encourage continuous improvement in wildfire risk assessment. Ultimately, I hope my work can serve as a small but meaningful step toward the improvement of data analysis models to tackle the growing threat of wildfires."
  },
  {
    "objectID": "Analysis.html#packages",
    "href": "Analysis.html#packages",
    "title": "Understanding Fire Severity in California",
    "section": "Packages",
    "text": "Packages\n\nlibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(rsample)\nlibrary(ggplot2)\nlibrary(naniar)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(recipes)\nlibrary(parsnip)\nlibrary(workflows)\nlibrary(yardstick)\nlibrary(tune)\nlibrary(dials)\nlibrary(broom)\n\n\n\n\nPackage\nFunction\n\n\n\n\nhere\nto easily load and save data\n\n\nreadr\nto import the CSV file data\n\n\ndplyr\nto massage and summarize data\n\n\nrsample\nto split data into training and test sets\n\n\nggplot2\nto create nice-looking and informative graphs\n\n\nrecipes\nto define and apply data transformation steps\n\n\nnaniar\nfor visualizing and handling missing data\n\n\nbroom\nfor visualizing and handling missing data\n\n\nlubridate\nto simplify working with dates and times"
  },
  {
    "objectID": "Analysis.html#data-description",
    "href": "Analysis.html#data-description",
    "title": "Understanding Fire Severity in California",
    "section": "Data Description",
    "text": "Data Description\nI am using the fire incident data available at the CAL FIRE website. CalFIRE (California Department of Forestry and Fire Protection) is California’s state agency responsible for fire protection in State Responsibility Areas (SRAs), which include over 31 million acres of wild lands and areas with significant natural resources. CAL FIRE also provides emergency services in cooperation with local governments and handles forest management, fire prevention, and resource protection.\nAll of the agency’s information comes from the fire lines and must be approved by the Incident Commander in charge of managing the incident prior to release. The data then gets compiled by their staff and published in their website for the public to access. In their incidents page, there is a Current Emergency Incidents section where it shows fire related information such as evacuation orders and acres burned through Earthstar Geographics powered by Esri, Genasys and Perimeter. However, since I needed the whole historical fire data set to perform my analysis , I collected the CSV file, they offered.\nThe main reasons why they collect the fire incident data includes fastening fire responsiveness, prevention of future fires, to inform the public through statistics, and to create an incentive for more funding towards fire prevention.\nOBS: It is important to note that the original excel file from CalFIRE contained significant amount of unnecessary raw data, and some wrong/outdated information. Therefore, this will be taken care of in our Data Wrangling section to ensure our Analysis is as precise and professional as possible.\n\noriginal_fire &lt;- readr::read_csv(here::here(\"mapdataall.csv\"))\n\n#original historical fire data from CalFIRE\n\n\nData Limitations\nCalFIRE makes it clear in their disclaimer that the data provided is intended for general reference rather than detailed scientific or emergency analysis. Therefore, important limitations must be considered when interpreting my model’s conclusions.\n1) Bias in data collection: One possible source of bias is the fact that CalFIRE reportedly shared that a significant amount of small fires could go under reported, even more so if the incident burns less than 10 acres. Furthermore, another source of bias can be explained by how I performed variable selection during the modeling process. Since I got the data before specifing my linear model, I must account for selection bias.\n2) Variables Mismatches: The outcome variable in our linear model, acres_burned is the only variable being used in my analysis to define the severity of the fire incidents. However, we are excluding so many crucial variables that influence a fire severity in the real-world such as structures destroyed, rural/urban differences, fire escalation, lack of firefighters, area accessibility, injuries, fatalities, more expensive/cheaper neighborhoods, total loss, and more. Plus, the variables incident_season and incident_county are very broad. I am not accounting for local weather, vegetation, source of fire, and human factors that could influence a fire behavior in the real-world.\n3) Model Transferrability: My model is only accounting for fires in California in the past 12 years. Hence, the model might be inappropriate for other regions that present different weather, air humidity, terrain, biodiversity, population demographics, government funding, and more."
  },
  {
    "objectID": "Analysis.html#data-wrangling",
    "href": "Analysis.html#data-wrangling",
    "title": "Understanding Fire Severity in California",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData Clean Up\nFirst of all, I must make sure this data set contains only historical fire data. CalFIRE predominantly oversee fire incidents but not exclusively. After some digging in incident_type, there were indeed two incidents reported as flood and one as earthquake. Hence, let’s remove them from our original data set and change the updated data set name for good practice.\n\nfire_data &lt;- original_fire %&gt;%\n  filter(!incident_type %in% c(\"Flood\", \"Earthquake\"))\n\nWe should be worried about the observations that have missing data for the incident_type variable because what if they are not a fire incident. Hence, I made sure to check the incident_name manually to ensure all of the observations in our data set were only fire incidents.\nFurthermore, since acres burned will be our response variable we must make sure we are only looking at fires that were already contained so that we know exactly how many acres were affected. Hence, the chunk below removes all the fire incidents that were still active as of May 2025. In this case, there were only three.\n\nfire_data &lt;- fire_data %&gt;%\n  filter(!is_active %in% c(\"Y\"))\n\nMoreover, since we are only looking at fire incidents back to twelve years ago, starting from 2013, I must apply a filter to remove the reported fires before that year. Also, I should apply a filter to remove the fires of 2025 later on when attempting to make inference about the fire severity trend over the years, because the graph might be misleading since we are in May and hence the count of fires will be significantly smaller than other years. For simplicity, I will apply those filters later on when I create the variable year. The reason why we picked 2013 as the starting year is because it is when the reporting data became regular and consistent. Before, there were only two fires incidents from 1969 and one from 2009.\nLastly, I must make sure the fire location, incident_county, is only considering California counties. After manually analyzing that variable, I did find that some reported fires are from Mexico and State of Nevada. This might have happened since the country and state are close to the border. Thus, we shall remove them manually:\n\nfire_data &lt;- fire_data %&gt;%\n  filter(!incident_county %in% c(\"Mexico, San Diego\", \"Nevada\", \"State of Nevada\", \"Nevada, Placer\"))\n\n\n\nVariable Selection\nNow, let’s take a look at the variables in the original data set provided by CalFIRE and understand the variables we are working with:\n\nhead(original_fire, 10)\n\n# A tibble: 10 × 23\n   incident_name  incident_is_final incident_date_last_u…¹ incident_date_created\n   &lt;chr&gt;          &lt;chr&gt;             &lt;dttm&gt;                 &lt;dttm&gt;               \n 1 Bridge Fire    Y                 2018-01-09 13:46:00    2017-10-31 11:22:00  \n 2 Pala Fire      Y                 2020-09-16 14:07:35    2009-05-24 14:56:00  \n 3 River Fire     Y                 2022-10-24 11:39:23    2013-02-24 08:16:00  \n 4 Fawnskin Fire  Y                 2013-04-22 09:00:00    2013-04-20 17:30:00  \n 5 Gold Fire      Y                 2013-05-01 07:00:00    2013-04-30 12:59:00  \n 6 Panther Fire   Y                 2022-10-24 11:40:03    2013-05-01 09:12:00  \n 7 Silverado Fire Y                 2013-05-01 17:15:00    2013-04-30 23:44:00  \n 8 Yellow Fire    Y                 2013-05-03 06:15:00    2013-05-01 02:01:00  \n 9 Summit Fire    Y                 2022-10-24 11:40:42    2013-05-01 12:38:00  \n10 Tres Pinos Fi… Y                 2013-05-03 18:45:00    2013-05-03 11:42:00  \n# ℹ abbreviated name: ¹​incident_date_last_update\n# ℹ 19 more variables: incident_administrative_unit &lt;chr&gt;,\n#   incident_administrative_unit_url &lt;lgl&gt;, incident_county &lt;chr&gt;,\n#   incident_location &lt;chr&gt;, incident_acres_burned &lt;dbl&gt;,\n#   incident_containment &lt;dbl&gt;, incident_control &lt;chr&gt;,\n#   incident_cooperating_agencies &lt;chr&gt;, incident_longitude &lt;dbl&gt;,\n#   incident_latitude &lt;dbl&gt;, incident_type &lt;chr&gt;, incident_id &lt;chr&gt;, …\n\ncolnames(original_fire)\n\n [1] \"incident_name\"                    \"incident_is_final\"               \n [3] \"incident_date_last_update\"        \"incident_date_created\"           \n [5] \"incident_administrative_unit\"     \"incident_administrative_unit_url\"\n [7] \"incident_county\"                  \"incident_location\"               \n [9] \"incident_acres_burned\"            \"incident_containment\"            \n[11] \"incident_control\"                 \"incident_cooperating_agencies\"   \n[13] \"incident_longitude\"               \"incident_latitude\"               \n[15] \"incident_type\"                    \"incident_id\"                     \n[17] \"incident_url\"                     \"incident_date_extinguished\"      \n[19] \"incident_dateonly_extinguished\"   \"incident_dateonly_created\"       \n[21] \"is_active\"                        \"calfire_incident\"                \n[23] \"notification_desired\"            \n\n\nWe can see there are some unnecessary variables that can be removed from this data set because they will be irrelevant to our analysis. More specifically:\n\nincident_is_final - I am only considering incidents that are final because days active is one of my predictor variables. Plus, there were only 3 fires that were active on the data set. They will be deleted in the chunks below.\nincident_date_last_update - Irrelevant for our analysis since fires considered are final.\nincident_county - I modified this variable to only include counties in California.\nincident_location - This column described the location of the fire, some cells described which freeway, other cells described off of which street, other cells gave more estimations (i.e. near the 4500 block of Chuckwagon Drive, Copperopolis). It would be very ambitious to consider this a variable.\nincident_containment - This column showed what percentage of the fire was contained, while 2794 out of 2864 were contained, the data showed 70 cells with missing data or less than 100% contained. However, this seems to be a variable that was not updated because all of those 70 fires were extinguished already, hence they have to have been contained.\nincident_longitude and incident_latitude - Irrelevant for our analysis, no need for the location of the fire to be this specific.\nincident_type - The original data set did contain two floods and one earthquake incidents that I deleted for the polished excel file. Also, this variable differentiated fire from wildfire. Since we are not getting that specific I decided to drop this variable as well.\nincident_date_extinguished and incident_dateonly_extinguished - I was going to use this variable to calculate for how long each fire has been active. However, after some digging, I found multiple inconsistencies in the reportedly duration of the fire. Some of them showed that they were active for more than a year! Plus, I found that if we consider incident_date_extinguished , near half of the fires supposedly were active for more than 60 days, which does not sound reasonable at all. However, I did not lose hope with this data set from CalFIRE because the incident_date_created has been precisely accurate with every fire I researched on my own. Similarly with acres_burned .\nincident_date_created - Repetitive, we already have incident_dateonly_created.\nincident_administrative_unit , incident_administrative_unit_url, incident_control , incident_cooperating_agencies , incident_id , incident_url, calfire_incident , notification_desired - Irrelevant for our analysis.\n\nLet’s create a new data set with those variables removed:\n\nfire_data &lt;- fire_data %&gt;%\n  select(-c(incident_is_final, incident_date_last_update, incident_location, incident_containment, incident_longitude, incident_latitude, incident_type, incident_date_extinguished, incident_dateonly_extinguished, incident_administrative_unit, incident_administrative_unit_url, incident_control, incident_cooperating_agencies, incident_id , incident_url, calfire_incident, notification_desired, incident_date_created, is_active))\n\n# To remove all the variables we deemed unecessary.\n\nNow that we have removed those unnecessary variables. Let’s extract other interesting variables from the given ones and perhaps change their names for our data set to look more professional. For example, from the date the fire was created we can extract the year, the month, and the season that fire originated. Hence, here are the variables we are creating from the given data:\n\nyear - what year the fire originated.\nmonth - what month the fire originated in numeric form.\nseason - what was the season when the fire originated.\nis_complex - a fire is called a “complex” when two or more fires burn in the same general area and are managed under a single incident command or unified command.\n\n\nfire_data &lt;- fire_data %&gt;%\n  mutate(\n    year = year(incident_dateonly_created),\n    month = factor(month(incident_dateonly_created),\n                   levels = 1:12,\n                   labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\",\n                              \"Jun\",\"Jul\", \"Aug\", \"Sep\", \"Oct\",\n                              \"Nov\", \"Dec\")),\n    season = case_when(\n      month %in% c(\"Dec\", \"Jan\", \"Feb\") ~ \"Winter\",\n      month %in% c(\"Mar\", \"Apr\", \"May\")  ~ \"Spring\",\n      month %in% c(\"Jun\", \"Jul\", \"Aug\")  ~ \"Summer\",\n      month %in% c(\"Sep\", \"Oct\", \"Nov\") ~ \"Fall\",\n      TRUE ~ NA_character_\n    ),\n    \n    is_complex = grepl(\"Complex\", incident_name)\n  )\n\nNow we can easily apply the filter to year and only consider fires from 2013 to 2024.\n\nfire_data &lt;- fire_data %&gt;%\n  filter(year &gt;= 2013)\n\nNow, let’s change some of the variables names for aesthetic:\n\nfire_data &lt;- fire_data %&gt;%\n  rename(\n    name = incident_name,\n    county = incident_county,\n    severity = incident_acres_burned,\n    date = incident_dateonly_created,\n  )\n\nFinally, let’s take a look at our updated and clean data set. We are now one step closer to perform our Analysis.\n\nhead(fire_data, 10)\n\n# A tibble: 10 × 8\n   name            county      severity date        year month season is_complex\n   &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;  &lt;lgl&gt;     \n 1 Bridge Fire     Shasta            37 2017-10-31  2017 Oct   Fall   FALSE     \n 2 River Fire      Inyo             407 2013-02-24  2013 Feb   Winter FALSE     \n 3 Fawnskin Fire   San Bernar…       30 2013-04-20  2013 Apr   Spring FALSE     \n 4 Gold Fire       Madera           274 2013-04-30  2013 Apr   Spring FALSE     \n 5 Panther Fire    Tehama          6896 2013-05-01  2013 May   Spring FALSE     \n 6 Silverado Fire  Napa              75 2013-04-30  2013 Apr   Spring FALSE     \n 7 Yellow Fire     Sonoma           125 2013-05-01  2013 May   Spring FALSE     \n 8 Summit Fire     Riverside       2956 2013-05-01  2013 May   Spring FALSE     \n 9 Tres Pinos Fire San Benito       354 2013-05-03  2013 May   Spring FALSE     \n10 306 Fire        Glenn            217 2013-05-01  2013 May   Spring FALSE     \n\ncolnames(fire_data)\n\n[1] \"name\"       \"county\"     \"severity\"   \"date\"       \"year\"      \n[6] \"month\"      \"season\"     \"is_complex\""
  },
  {
    "objectID": "Analysis.html#exploratory-data-analysis",
    "href": "Analysis.html#exploratory-data-analysis",
    "title": "Understanding Fire Severity in California",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nIn this section, we will explore our variables by looking at their distributions and relationship between our categorical variables and our response variable, severity.\n\nMissing Data\nBefore exploring our variables, we must address how much of the data is missing and how can we tackle this problem.\n\n    fire_data |&gt;\n      miss_var_summary()\n\n# A tibble: 8 × 3\n  variable   n_miss pct_miss\n  &lt;chr&gt;       &lt;int&gt;    &lt;num&gt;\n1 severity       51    1.80 \n2 county         10    0.354\n3 name            0    0    \n4 date            0    0    \n5 year            0    0    \n6 month           0    0    \n7 season          0    0    \n8 is_complex      0    0    \n\n\nHence, we have 61 missing data points in total in our data. More precisely, 51 reports missing for severity (acres burned) and 10 reports missing for county (fire incident county), which represents about 1.80% and 0.35% of those variables, respectively, total reported data. Therefore, this is a good sign that our data set has barely any data missing. We could even visualize how insignificant the missingness is related to our overall data,\n\nvis_miss(fire_data, cluster = TRUE, sort_miss = TRUE)\n\n\n\n\n\n\n\n\n\ngg_miss_upset(fire_data,\n              nsets = 9)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nTherefore, approximately 2% of the data had missing values. I opted to remove those rows. This avoids the need for potentially biased imputation of the target variable, with methods such as Mean/Median/Mode, linear model, or softImpute. The small proportion of missing data will not significantly impact our overall analysis.\nMoreover, we it is plausible to ignore the missing data because the “upset” plot indicated that the variables do not tend to be missing together. Hence, this strengthen, but does not prove, our assumption that the data can be treated as Missing Completely at Random (MCAR).\n\nfire_data &lt;- fire_data %&gt;% \n  drop_na()\n\n\n    fire_data |&gt;\n      miss_var_summary()\n\n# A tibble: 8 × 3\n  variable   n_miss pct_miss\n  &lt;chr&gt;       &lt;int&gt;    &lt;num&gt;\n1 name            0        0\n2 county          0        0\n3 severity        0        0\n4 date            0        0\n5 year            0        0\n6 month           0        0\n7 season          0        0\n8 is_complex      0        0\n\n\nHence, we now have a clean data set with 2,766 fire incidents across California with no missing data.\n\n\nData Split\nFurthermore, we should split the data into a training set (fire_train) and test set (fire_test). The recommended amount of data in the training set is roughly 80% for a random split, thus we shall follow that. We must apply this split for several reasons, more importantly, to prevent over fitting and to evaluate/compare models accurately. Our test set is extremely important because it acts like “new” data, helping us check if the trained model generalizes. Also, since we are planning on making inference claims, we must make sure our model’s conclusions are not specific to the training set only.\n\nset.seed(437)\n\nn &lt;- nrow(fire_data)\ntrain_indices &lt;- sample(n, size = floor(0.8*n))\n\nfire_train &lt;- fire_data[train_indices,]\nfire_test &lt;- fire_data[-train_indices,]\n\n\n\nExploring Variables\nLet’s take a look at our response variable:\n\nfire_train |&gt;\n  summarize(\n    num_total = n(),\n    mean = mean(severity),\n    sd = sd(severity),\n    min = min(severity),\n    Q1 = quantile(severity, 0.25),\n    median = median(severity),\n    Q3 = quantile(severity, 0.75),\n    max = max(severity)\n    )\n\n# A tibble: 1 × 8\n  num_total  mean     sd   min    Q1 median    Q3     max\n      &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1      2212 4538. 38586.     0    30     83  334. 1032648\n\n\nHence, this tell us that from the 2,212 fires observed in our training set, 50% of them burned between 30 and 338 acres, with 25% of fires burning less than 30 acres and 25% of fires burning more than 338 acres. Notice how the average (mean) of acres burned is around 4,320 acres which is much higher than the median of 83 acres. This implies that half of the fires burn 85 acres or less, however there are a few massive fires (outliers) that are significantly shifting the average. Plus, the standard deviation is extremely high, around 37,221 acres. This suggests a very strong right skewness for the distribution of severity .\nFurthermore, notice how we have observations of fires that burned 0 acres and a fire that was able to reach the incredible 1,032,648 acres mark. This extreme difference between fire severity explain the variability in the data, together with he standard deviation. In fact, in other to show the plot of severity we need to apply a Logarithmic Scale to compress those extreme values and allow us to visualize and interpret the full distribution of severity .\n\nggplot(fire_train, aes(x = severity)) +\n  geom_histogram(bins = 70, fill = \"purple\", color = \"black\") +\n  scale_x_log10() +\n  labs(title = \"Distribution of Fire Severity (Log Scale)\", x = \"Severity\", y = \"Count of Fires\") +\n  theme_minimal()\n\nAs predicted, severity has a clear right-skewed distribution. Here is how to interpret the Log Scale:\n\n\n\nNotation\nAcres Burned\n\n\n\n\n1e+01\n10\n\n\n1e+02\n100\n\n\n1e+03\n1,000\n\n\n1e+04\n10,000\n\n\n1e+05\n100,000\n\n\n1e+06\n1,000,000\n\n\n\nHence, the histogram reveals that the great majority of fires in our training set burn between 10 to 1,000 acres. Including an extreme “high peak” at a little before 100 acres, which explains the median of 83 from our Summary.\nNow, let’s compare the severity of the fires by season of the year:\n\nggplot(fire_train, aes(x = season, y = severity)) +\n  geom_boxplot(fill = \"purple\") +\n  scale_y_log10() +\n  labs(\n    title = \"Severity of Fires by Season (Log Scale)\",\n    x = \"Season\",\n    y = \"Severity\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHence, we can see that the median fire severity appears consistent across the seasons, with about half of the fires in each season burned around or a little below 100 acres. However, Fall and Summer demonstrate to have the most outliers (extreme fire events) with severity well above 10,000 acres and even reaching up to over 1,000,000 acres, in Summer. This strongly suggests that while the typical size of fire in our training set do not vary much across seasons, the most severe fires tend to happen during Summer and Fall.\n\nfire_train |&gt;\ngroup_by(season) |&gt;\nsummarize(\nnum_total = n(),\nmean = mean(severity),\nsd = sd(severity),\nmin = min(severity),\nQ1 = quantile(severity, 0.25),\nmedian = median(severity),\nQ3 = quantile(severity, 0.75),\nmax = max(severity)\n)\n\n# A tibble: 4 × 9\n  season num_total  mean     sd   min    Q1 median    Q3     max\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Fall         463 3470. 20698.     0  30     82    378   379895\n2 Spring       269  511.  2122.     0  23     70    197    30274\n3 Summer      1404 5647. 46301.     0  31.8   86    351. 1032648\n4 Winter        76 4810. 32338.     0  23.8   53.5  398   281893\n\n\nMore precisely, notice how many more fires happened in California during Summer compared to the other seasons, 1,400 fire incidents representing around 63% of the fires in our training set. Hence, this suggests that fire incidents in our training set tend to happen significantly more often during the Summer than other seasons.\nWhat if we look at the fire severity over the years. However, we must be careful to not include the fire incidents from 2025 since we are only in the middle of the year and the amount of fires will most likely be less than other years. Hence, including those could cause a misleading plot. Thus, we will subset the rows of fire_train to not include the incidents from 2025.\nNote: We did not just remove the fires from 2025 from fire_data , the whole data set, because the data was useful for us to plot the severity distribution and to analyze its relationship with season .\n\nfire_over_years &lt;- fire_data |&gt; \n  filter(year &lt;= 2024)\n\n\nfire_over_years |&gt;\ngroup_by(year) |&gt;\nsummarize(\nnum_total = n(),\nmean = mean(severity),\nsd = sd(severity),\nmin = min(severity),\nQ1 = quantile(severity, 0.25),\nmedian = median(severity),\nQ3 = quantile(severity, 0.75),\nmax = max(severity)\n)\n\n# A tibble: 12 × 9\n    year num_total   mean     sd   min    Q1 median    Q3     max\n   &lt;dbl&gt;     &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  2013       138  3599. 22369.     0  50    124    498   257314\n 2  2014        73  4063. 13329.     0  75    274   1651    97717\n 3  2015        95  4315. 18745.     0  51    110    408.  151623\n 4  2016       153  2954. 12703.     0  45    114    554   132127\n 5  2017       420  2993. 16449.     0  29.8   74    400   281893\n 6  2018       298  5134. 30591.     0  30     78.5  312   410203\n 7  2019       258  1107.  6088.     9  33     90.5  262.   77758\n 8  2020       242 12142. 78823.     2  56.2  161    693  1032648\n 9  2021       178 12863. 78833.    10  43.2  109    460.  963309\n10  2022       131  2116.  9133.    10  37.5   93    321    76788\n11  2023       121  2669. 12948.     5  32     70    296    95107\n12  2024       606  1690. 18079.     2  17     43    150   429603\n\n\n\nggplot(\n  data = fire_over_years,\n  mapping = aes(\n    x = year, \n    y = severity\n    )\n  ) +\n  \n  geom_point(\n    alpha = 0.2\n    ) +\n  \n  geom_smooth(method = \"loess\", se = FALSE, color = \"tomato\") +\n  \n  scale_y_log10() +\n  scale_x_continuous(breaks = seq(2013, 2024, 1)) +\n  \n  labs(title = \"Fire Severity Over Time\",\n       x = \"Years\",\n       y = \"Severity (Log Scale)\"\n       )\n\nHence, this plot suggests that year have a slight effect on severity .\nOBS: Our choice of alpha here, “alpha = 0.2” is to reduce the opacity of the data points and make them more readable.\n\nggplot(data = fire_train, \n       mapping = aes(\n         x = season,\n         y = severity\n         )\n       ) +\n    scale_y_log10() +\n  geom_boxplot(fill = \"purple\") + \n  geom_point(aes(color = year), \n             alpha = 0.8\n             )"
  },
  {
    "objectID": "Analysis.html#modeling",
    "href": "Analysis.html#modeling",
    "title": "Understanding Fire Severity in California",
    "section": "Modeling",
    "text": "Modeling\nBased on our Exploratory Data Analysis and the fact that our response variable severity is numeric with variable predictors categorical, I decided to perform my modeling with Multiple Linear Regression."
  },
  {
    "objectID": "Analysis.html#insights",
    "href": "Analysis.html#insights",
    "title": "Understanding Fire Severity in California",
    "section": "Insights",
    "text": "Insights\nTherefore, we are now able to make inference and answer our questions that motivated my interest for this project. We were able to arrive at these results by applying Multiple Linear Regression with a Log transformation to help with the skewness of severity.\n\nDoes the severity of a fire depend on the season of the year?\nYes. During Exploratoy Data Analysis, we could see through the box plots how fires more severe tend to happen during Fall and Summer. However, we were not able to prove that relationship through our model. Although, our model was still able to determine that there is dependency of a fire on one specific season; Spring is shown to burn 0.69 fewer acres than in the Fall.\nAre wildfires in California worsening over time in terms of severity?\nNo. In fact, our model showed very strong significance that fires in California are becoming less severe over time, reducing by 0.91 acres per year.\nDoes a Complex fire in California influence fire severity?\nYes. In fact, our model successfully showed that Complex fires are the biggest predictor variable of a fire severity in California. They burn in total 100 acres more than regular fires."
  },
  {
    "objectID": "Analysis.html#limitations-and-future-work",
    "href": "Analysis.html#limitations-and-future-work",
    "title": "Understanding Fire Severity in California",
    "section": "Limitations and Future Work",
    "text": "Limitations and Future Work\nOur model was able to make some inference, but it is not the greatest, there are definitely some limitations to be aware of.\nFirst off, it’s not great at predicting the most extreme fires, the ones that burn huge numbers of acres. These are rare, but when they happen, they can be very different from the more typical fires. Our model tends to underpredict those big ones, probably because there’s not enough of them in the training data to really learn from. Also, we used a log transformation to make the model work better, which helps with scale, but it can also make interpretation a bit less intuitive.\nAnother limitation is the kind of data we’re working with. All of our predictors are categorical, so we’re missing out some relevant numerical variables that might have a strong relationship with severity like temperature, humidity, wind speed, or vegetation type. If someone were to collect more detailed weather or geographic data for each incident, the model could be way more accurate.\nAs for assumptions, we used a multiple linear regression model, which assumes linear relationships, normal residuals, constant variance, etc. These assumptions don’t always perfectly hold up in real-world wildfire data, especially with such skewed outcomes like severity.\nTalking about ethics, it’s worth thinking about how this data is used. This content is merely illustrative and people should not use it to allocate resources, for example. Although, if this model gets improved and becomes more accurate then it’s important to make sure under-resourced areas aren’t overlooked just because the model predicts less severe fires in that location. Also, because if the data collection itself missed smaller or less-reported fires (especially in rural or underserved regions), it could create bias in the results.\nSo overall, the model is helpful for our illustrative purpose of showcasing Modern Data Analysis skills, but definitely not to be applied to the real world fire incidents in California. I believe there’s room for improvement both in the data collection and the analysis. Hence, I plan to keep working on this model to make it even more accurate and precise."
  },
  {
    "objectID": "Analysis.html#reflection",
    "href": "Analysis.html#reflection",
    "title": "Understanding Fire Severity in California",
    "section": "Reflection",
    "text": "Reflection\nThis project contributed significantly to my understanding of statistical learning and all the steps that one must go through to so that we can answer inference questions from a raw data set, including Research, Data Wrangling, EDA, and Modeling. Plus, I have learned more about the fires in California by looking at their data through distributions and plots. Also, I learned about different associations between the characteristics of a fire, which we called variables.\nMath 437 is one of the most important, if not the most important, courses I have taken at CSUF. This is because this class is a big review of all statistical concepts learned from previous Undergrad math courses, while introducing new advanced statistical approaches, and also while teaching how to apply those old and new concepts in R for data analysis. In my opinion, what makes this class so important, is that it makes us actually work with the data from scratch and understand what those numbers, variables, distributions, associations, predictions, and models actually mean.\nHence, I have definitely grown as a mathematician, statistician, and data scientist after taking this course because I had to study as the first one, apply the knowledge as the second, and think analytically like the third. I hope to continue growing in all three areas, as it will contribute to my career as an actuary. I hope to apply the knowledge I have learned from this class in my internship at Mercury Insurance. Not only the technical skills in R, but also the mindset of a true data analyst."
  },
  {
    "objectID": "Final Project Website/blog/first-post/index.html",
    "href": "Final Project Website/blog/first-post/index.html",
    "title": "First Post",
    "section": "",
    "text": "Sed risus ultricies tristique nulla aliquet. Neque volutpat ac tincidunt vitae semper quis lectus nulla.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Enim sed faucibus turpis in eu mi bibendum neque. Ac orci phasellus egestas tellus rutrum tellus pellentesque eu. Velit sed ullamcorper morbi tincidunt ornare massa. Sagittis id consectetur purus ut faucibus pulvinar elementum integer. Tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Lobortis feugiat vivamus at augue eget arcu. Aliquam ut porttitor leo a diam sollicitudin tempor id eu. Mauris a diam maecenas sed enim ut sem viverra aliquet. Enim ut tellus elementum sagittis vitae et leo duis. Molestie at elementum eu facilisis sed odio morbi quis commodo. Sapien pellentesque habitant morbi tristique senectus. Quam vulputate dignissim suspendisse in est. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.\nVelit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Viverra mauris in aliquam sem fringilla ut morbi tincidunt augue. Tortor at auctor urna nunc id. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Aliquet nibh praesent tristique magna sit amet purus. Tristique senectus et netus et malesuada fames ac turpis. Hac habitasse platea dictumst quisque. Auctor neque vitae tempus quam pellentesque nec nam aliquam. Ultrices tincidunt arcu non sodales neque sodales ut etiam. Iaculis at erat pellentesque adipiscing. Cras tincidunt lobortis feugiat vivamus. Nisi est sit amet facilisis magna etiam. Pharetra pharetra massa massa ultricies mi quis hendrerit. Vitae sapien pellentesque habitant morbi tristique senectus. Ornare aenean euismod elementum nisi quis eleifend quam adipiscing vitae."
  },
  {
    "objectID": "Final Project Website/blog/third-post/index.html",
    "href": "Final Project Website/blog/third-post/index.html",
    "title": "Third Blog Post",
    "section": "",
    "text": "The source for any page in your website could also be a Jupyter Notebook. This one is third-post/index.ipynb.\nHere’s an example I borrowed from the Seaborn docs:\n\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Plot the distribution of clarity ratings, conditional on carat\nsns.displot(\n    data=diamonds,\n    x=\"carat\", hue=\"cut\",\n    kind=\"kde\", height=4, aspect=1.5,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\",   \n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erick Doherty Claudino, B.A.",
    "section": "",
    "text": "I am a self-motivated and resilient student majoring in Mathematics with a concentration in Actuarial Science. I have a strong passion for the actuarial field and recognize its significance in addressing complex issues by applying mathematics, statistics, and risk management, bringing clarity to chaos. My goal is to become an actuary and make a meaningful contribution to society by protecting individuals."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Erick Doherty Claudino, B.A.",
    "section": "Education",
    "text": "Education\nBachelor’s Degree in Mathematics - Actuarial Science Concentration | California State Univeristy, Fullerton"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Erick Doherty Claudino, B.A.",
    "section": "Experience",
    "text": "Experience\nCalifornia Department of Insurance | Actuarial Intern | June 2024 - Aug 2024\nMercury Insurance | Actuarial Intern | June 2025 - Aug 2025"
  },
  {
    "objectID": "Analysis.html#multiple-linear-regression",
    "href": "Analysis.html#multiple-linear-regression",
    "title": "Understanding Fire Severity in California",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nlm_model &lt;- lm(severity ~ season + year + is_complex, \n               data = fire_train)\n\nsummary(lm_model)\n\n\nCall:\nlm(formula = severity ~ season + year + is_complex, data = fire_train)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-79753  -3897  -3642  -1706 959301 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -49045.55  474046.23  -0.103    0.918    \nseasonSpring    -1497.26    2857.16  -0.524    0.600    \nseasonSummer     1968.80    1995.41   0.987    0.324    \nseasonWinter     1764.11    4630.96   0.381    0.703    \nyear               25.28     234.75   0.108    0.914    \nis_complexTRUE  75846.37    5879.64  12.900   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 37210 on 2206 degrees of freedom\nMultiple R-squared:  0.07209,   Adjusted R-squared:  0.06999 \nF-statistic: 34.28 on 5 and 2206 DF,  p-value: &lt; 2.2e-16\n\n\nThus, we are modeling severity based on season, year, and is_complex. Our intercept \\((-49046)\\) is the expected severity of a non-complex fire when it happens in the Fall season.\nThe coefficients seasonSpring , seasonSummer, seasonWinter reveal the expected difference in severity when compared to the Fall season. Notice how none of the p-values are significant, hence \\(p &gt; 0.05\\). Thus, there is no sufficient evidence to conclude that fire severity depends by season after controlling for year and is_complex .\nThe coefficient of year is \\(25.28\\) would imply that the severity of a fire increases by \\(25.28\\) acres per year. However, there is not sufficient evidence to conclude this since the p-value = \\(0.914 &gt; 0.05\\).\nThe coefficient of is_complexTRUE is \\(75,846\\). Since the p-value is extremely small, p &lt; 2e-16. We have very strong evidence that Complex fires burn on average \\(75,846\\) acres more than regular fires. Hence, making is_complex our only statiscally significant predictor variable in this model.\nTo measure our model performance, we can look at the given measures in our model summary:\n\nResidual Standard Error (RSE): Measures the average size of error in our model, how far off our predictions are, given by\n\\[\n\\text{RSE} = \\sqrt{ \\frac{1}{n - p - 1} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 }\n\\]\nR-squared: Measures how much of the variation in our response variable is explained by the predictors in our model, given by\n\\[\nR^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}\n\\]\nwhere,\n\\[\nSS_{\\text{res}} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2, \\quad SS_{\\text{tot}} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n\\]\n\nHence, our \\(RSE\\) is telling us that the average prediction error in our model is around 37,210 acres. This is not good because the error is too high. Plus, our \\(R^2\\) tells us that our model can only explain 7.2% variation in severity. This is also not good because it means that our model does not tell us much.\nOur model lack of success must be because of how strongly skewed the distribution of severity is. Hence, let’s apply a log transformation with the goal to reduce the skewness and improve our model. However, we must then not include those fires which severity was 0, hence smaller fires that burned 0 acres.\nNote: This is because log(0) is undefined.\nThus by doing so, we are leaving behind from our modeling 17 fire observations, as observed in the chunk below.\n\nfire_train |&gt; \n  select(name, severity) |&gt; \n  filter(severity == 0)\n\n# A tibble: 17 × 2\n   name                                        severity\n   &lt;chr&gt;                                          &lt;dbl&gt;\n 1 Pressley Fire (Central LNU Complex)                0\n 2 Oroville Spillway                                  0\n 3 Mokelumne Fire                                     0\n 4 Gorman Fire                                        0\n 5 Grape Fire                                         0\n 6 Soda Fire                                          0\n 7 Orleans Complex                                    0\n 8 Cristianitos Fire                                  0\n 9 Happy Camp Complex                                 0\n10 King Incident                                      0\n11 Salmon-August Complex                              0\n12 Klamath Fire                                       0\n13 Rattlesnake Fire                                   0\n14 Warner Mountain Lightning                          0\n15 Hathaway Fire                                      0\n16 Clear Complex (merged into Eclipse Complex)        0\n17 Creek Fire                                         0\n\n\nNote: This is because log(0) is undefined.\n\nfire_train_non_zero &lt;- fire_train %&gt;%\n  filter(severity &gt; 0)\n\nlm_model_log &lt;- lm(log10(severity) ~ season + year + is_complex, \n                   data = fire_train_non_zero)\n\nsummary(lm_model_log)\n\n\nCall:\nlm(formula = log10(severity) ~ season + year + is_complex, data = fire_train_non_zero)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7706 -0.6205 -0.1881  0.3998  3.9298 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    90.5799067 10.8830624   8.323  &lt; 2e-16 ***\nseasonSpring   -0.1598536  0.0655252  -2.440   0.0148 *  \nseasonSummer   -0.0005002  0.0457450  -0.011   0.9913    \nseasonWinter    0.0109204  0.1065341   0.103   0.9184    \nyear           -0.0438028  0.0053894  -8.128 7.25e-16 ***\nis_complexTRUE  2.0062501  0.1431829  14.012  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8507 on 2189 degrees of freedom\nMultiple R-squared:  0.1145,    Adjusted R-squared:  0.1125 \nF-statistic:  56.6 on 5 and 2189 DF,  p-value: &lt; 2.2e-16\n\n\nTherefore, this log transformation increased the model’s performance and efficiency. This is notable since now we have three statistically significant predictors.\n\nseasonSpring has a p-value of \\(0.0148 &lt; 0.05\\). Hence, there is sufficient evidence to conclude that fires in the Spring burn, in total, \\(10^{-0.16} = 0.69\\) fewer acres than fires in the Fall.\nyear has a p-value of 7.25e-16. Therefore, there is very strong evidence suggesting that fires are getting smaller each year, more precisely, reducing \\(10^{-0.04} = 0.91\\) acres per year.\nis_complexTRUE has a p-value of less than 2e-16. Thus, there is very strong evidence claiming that Complex fires burn, in total, \\(10^2 = 100\\) more acres than regular fires.\n\nHence, this model tell us that Complex fires are the biggest predictor variable of severity. Fires are slightly decreasing in size/severity over time. Lastly, only Spring season shows a significant difference in size/severity of fires than in the Fall.\nFor our model’s performance, \\(RSE = 0.8507\\) and \\(R^2 = 0.1145\\). Hence, the average prediction error in our model is \\(10^{0.8507} = 7.09\\) acres. This is a much better error than before. Plus, our \\(R^2\\) tells us that our model can only explain \\(11.45\\)% variation in severity. This is still not that good because it means that our model does not tell us much.\nNow, we must fit our model to the test set (fire_set) since we want to answer our inference questions. We must create a subset of the test set though to filter the fires that did not burned 0 acres, call it fire_test_non_zero.\n\nfire_test_non_zero &lt;- fire_test %&gt;%\n  filter(severity &gt; 0)\n\nThus, we now predict on the test set.\n\nfire_test_non_zero &lt;- fire_test_non_zero %&gt;%\n  \n  mutate(predicted_log_severity = \n           predict(lm_model_log, \n                   newdata = fire_test_non_zero))\n\n\nfire_test_non_zero &lt;- fire_test_non_zero %&gt;%\n  \n  mutate(log_severity = log10(severity))\n\nmetrics(fire_test_non_zero, truth = log_severity, \n        estimate = predicted_log_severity)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.860 \n2 rsq     standard      0.0663\n3 mae     standard      0.654"
  }
]